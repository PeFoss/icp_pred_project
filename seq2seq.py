# -*- coding: utf-8 -*-
"""seq2seq.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ES11kzPviZAIJ6kLwVhmLRoEmKJFFZLt
"""

import numpy as np
import pandas as pd
import math

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/ChopraLab/Project1-Data/test1.csv')
df = df[::3]
df.reset_index(drop=True, inplace=True)

import tensorflow as tf

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

def process_data(column, sequence_length):
  x = df.drop(columns=['ICP'], axis=1)

  x = x[:round(x.values.shape[0]/sequence_length)*sequence_length]
  x = x.values.reshape(int(x.values.shape[0]/sequence_length), sequence_length, 2)
  y = df['ICP']
  y = y[:round(y.values.shape[0]/sequence_length)*sequence_length]
  y = y.values.reshape(int(y.values.shape[0]/sequence_length), sequence_length)
  Xtrain, Xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, random_state=42)
  ytrain = ytrain.reshape(ytrain.shape[0], ytrain.shape[1], 1)
  Xtrain = Xtrain.reshape(Xtrain.shape[0], Xtrain.shape[1], 2)
  Xtest = Xtest.reshape(Xtest.shape[0], Xtest.shape[1], 2)
  return Xtrain, Xtest, ytrain, ytest

scaler = MinMaxScaler()
df = scaler.fit_transform(df)
df = pd.DataFrame(df, columns = [['MAP', 'ICP', 'nICP']])
Xtrain, Xtest, ytrain, ytest = process_data('MAP', 100)

class Encoder(tf.keras.layers.Layer):
  def __init__(self, num_features, hidden_size, dropout):
    super(Encoder, self).__init__()
    self.dropout = tf.keras.layers.Dropout(dropout)
    self.rnn = tf.keras.layers.LSTM(hidden_size, input_shape=(None, num_features), return_sequences=True, return_state=True)

  def call(self, x):
    x = self.dropout(x)

    encoder_outputs, hidden_state, cell_state = self.rnn(x)

    return encoder_outputs, hidden_state, cell_state


class Attention(tf.keras.layers.Layer):
  def __init__(self):
    super(Attention, self).__init__()
    self.mha = tf.keras.layers.MultiHeadAttention(key_dim=3, num_heads=1)
    self.layernorm = tf.keras.layers.LayerNormalization()
    self.add = tf.keras.layers.Add()

  def call(self, x, context):

    attn_output = self.mha(
        query=x,
        value=context,
        )

  
    x = self.add([x, attn_output])
    x = self.layernorm(x)

    return x

class Decoder(tf.keras.layers.Layer):
  def __init__(self, num_features, hidden_size, dropout):
    super(Decoder, self).__init__()
    self.dropout = tf.keras.layers.Dropout(dropout)
    self.rnn = tf.keras.layers.LSTM(hidden_size, input_shape=(None, num_features), return_sequences=True, return_state=True)
    self.dense = tf.keras.layers.Dense(1, activation='linear')
    self.attention = Attention()
  
  def call(self, x, encoder_output, hidden, cell):
    x = self.dropout(x)

    rnn_output, hidden, cell = self.rnn(x, initial_state=[hidden, cell])

    dense_input = self.attention(rnn_output, encoder_output)

    output = self.dense(dense_input)

    return output


class Seq2Seq(tf.keras.Model):
  def __init__(self, num_features_encoder, num_features_decoder, hidden_size, dropout):
    super(Seq2Seq, self).__init__()    
    self.encoder = Encoder(num_features_encoder, hidden_size, dropout)
    self.decoder = Decoder(num_features_decoder, hidden_size, dropout)

  def call(self, inputs):
    original, target = inputs

    enc_outputs, hidden, cell = self.encoder(original)

    outputs = self.decoder(target, enc_outputs, hidden, cell)

    return outputs

model = Seq2Seq(2, 1, 1024, 0.5)

model.compile(optimizer=tf.keras.optimizers.Adam(tf.keras.optimizers.schedules.ExponentialDecay(
      initial_learning_rate=1e-3,
      decay_steps=100000,
      decay_rate=0.98)), loss=tf.keras.losses.MeanSquaredError())

ytarget = ytrain[:, :-1, :]
ytarget = np.concatenate((np.zeros([ytrain.shape[0], 1, ytrain.shape[2]]), ytarget), axis=1)

model.fit([Xtrain, ytarget], ytrain, epochs=10, batch_size=64)

lr = tf.keras.optimizers.schedules.ExponentialDecay(
      initial_learning_rate=1e-3,
      decay_steps=100000,
      decay_rate=0.98)
batch_size = 32
epochs = 5

hidden_size = 1024
input_size_encoder = 2
input_size_decoder = 1
output_size = 1
num_layers = 1
dropout_rate = 0.5
tf.random.set_seed(1234)
np.random.seed(1234)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense

encoder_inputs = Input(shape=(None, input_size_encoder))
encoder = LSTM(hidden_size, return_state=True, return_sequences=True)
encoder_outputs, state_h, state_c = encoder(encoder_inputs)

# We discard `encoder_outputs` and only keep the states.
encoder_states = [state_h, state_c]

# Set up the decoder, using `encoder_states` as initial state.
decoder_inputs = Input(shape=(None, input_size_decoder))
# We set up our decoder to return full output sequences,
# and to return internal states as well. We don't use the 
# return states in the training model, but we will use them in inference.
decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_inputs,
                                     initial_state=encoder_states)
decoder_dense = Dense(output_size, activation=None)
decoder_outputs = decoder_dense(decoder_outputs)

# Define the model that will turnimage/pjpeg;base64,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
# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

model.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='mse', metrics=['mse'])

ytarget = ytrain[:, :-1, :]
ytarget = np.concatenate((np.zeros([ytrain.shape[0], 1, ytrain.shape[2]]), ytarget), axis=1)


model.fit([Xtrain, ytarget], ytrain,
          batch_size=batch_size,
          epochs=epochs, 
          validation_data=(Xtest, ytest))

encoder_model = Model(encoder_inputs, encoder_states)
decoder_state_input_h = Input(shape=(hidden_size,))
decoder_state_input_c = Input(shape=(hidden_size,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
decoder_outputs, state_h, state_c = decoder_lstm(
    decoder_inputs, initial_state=decoder_states_inputs)
decoder_states = [state_h, state_c]
decoder_outputs = decoder_dense(decoder_outputs)
decoder_model = Model(
    [decoder_inputs] + decoder_states_inputs,
    [decoder_outputs] + decoder_states)

def seq2seq(input_test):
  state = encoder_model.predict(input_test)

  target_seq = np.zeros((input_test.shape[0], 1, 1))

  target_len = input_test.shape[1]
  print(target_len)

  stop_condition = False 

  outputs = []

  seq_index = 0

  while not stop_condition:
    output_tokens, h, c = decoder_model.predict(
            [target_seq] + state)

    state = [h, c]

    outputs.append(output_tokens)

    target_seq = output_tokens

    seq_index += 1

    if seq_index == target_len:
      stop_condition = True

    if stop_condition:
      break

  return np.concatenate(outputs, axis=1)



outputs = seq2seq(Xtest)

print("outputs", outputs)

print("shape outputs", outputs.shape)

# outputs, ytest, Xtest = outputs.flatten(), ytest.flatten(), Xtest.flatten()
ytest, Xtest = ytest.flatten(), Xtest[:, :, 1].flatten()
ytest = pd.DataFrame(ytest)
predictX = pd.DataFrame(outputs)
Xtest = pd.DataFrame(Xtest)
df_predicts = pd.concat([ytest, predictX, Xtest], axis=1)
df_predicts.columns = ['Y test', 'Y Predicts', 'X test']
df_predicts.set_index('X test', inplace=True)

df_predicts

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')

ax = sns.scatterplot(data=df_predicts[::100], x='Y Predicts', y='Y test')
ax.set(xlim=(0, 0.8))

sns.lineplot(data = df_predicts)

ax = sns.lineplot(data=df_predicts.iloc[15100:15200])
ax.set(ylim=(0, 0.8))

